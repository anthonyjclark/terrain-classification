{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# python image_stitching_simple.py --images images/scottsdale/test --output output3.png --crop 1\n",
    "# python image_stitching_simple.py --images images/scottsdale --output output4.png --crop 1\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "# import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# # construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--images\", type=str, required=True,\n",
    "#                 help=\"path to input directory of images to stitch\")\n",
    "# ap.add_argument(\"-o\", \"--output\", type=str, required=True,\n",
    "#                 help=\"path to the output image\")\n",
    "# ap.add_argument(\"-c\", \"--crop\", type=int, default=0,\n",
    "#                 help=\"whether to crop out largest rectangular region\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "args = {\n",
    "    \"images\": \"/home/ajc/Documents/terrain-classification/image-data/tmp\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# grab the paths to the input images and initialize our images list\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = sorted(list(paths.list_images(args[\"images\"])))\n",
    "images = []\n",
    "\n",
    "# loop over the image paths, load each one, and add them to our\n",
    "# images to stitch list\n",
    "for imagePath in imagePaths:\n",
    "    image = cv2.imread(imagePath)\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6aa689ebfff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB));\n",
    "ax2.imshow(cv2.cvtColor(images[1], cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "class ViewSynthisizer:\n",
    "    def __init__(self):\n",
    "        # determine if we are using OpenCV v3.X and initialize the\n",
    "        # cached homography matrix\n",
    "        self.isv3 = imutils.is_cv3(or_better=True)\n",
    "        self.cachedH = None\n",
    "\n",
    "    def stitch(self, images, ratio=0.75, reprojThresh=4.0):\n",
    "        # unpack the images\n",
    "        (imageB, imageA) = images\n",
    "\n",
    "        # if the cached homography matrix is None, then we need to\n",
    "        # apply keypoint matching to construct it\n",
    "        if self.cachedH is None:\n",
    "            # detect keypoints and extract\n",
    "            (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "            (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    "\n",
    "            # match features between the two images\n",
    "            M = self.matchKeypoints(kpsA, kpsB,\n",
    "                featuresA, featuresB, ratio, reprojThresh)\n",
    "\n",
    "            # if the match is None, then there aren't enough matched\n",
    "            # keypoints to create a panorama\n",
    "            if M is None:\n",
    "                return None\n",
    "\n",
    "            # cache the homography matrix\n",
    "            self.cachedH = M[1]\n",
    "\n",
    "        # apply a perspective transform to stitch the images together\n",
    "        # using the cached homography matrix\n",
    "        result = cv2.warpPerspective(imageA, self.cachedH,\n",
    "            (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "        result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "        \n",
    "        self.cachedH = None\n",
    "\n",
    "        # return the stitched image\n",
    "        return result\n",
    "\n",
    "    def detectAndDescribe(self, image):\n",
    "        # convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and extract features from the image\n",
    "        descriptor = cv2.ORB_create()\n",
    "        (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "        # convert the keypoints from KeyPoint objects to NumPy\n",
    "        # arrays\n",
    "        kps = np.float32([kp.pt for kp in kps])\n",
    "\n",
    "        # return a tuple of keypoints and features\n",
    "        return (kps, features)\n",
    "\n",
    "    def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB,\n",
    "        ratio, reprojThresh):\n",
    "        # compute the raw matches and initialize the list of actual\n",
    "        # matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    "\n",
    "        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "            # ensure the distance is within a certain ratio of each\n",
    "            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "        # computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    "\n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
    "                reprojThresh)\n",
    "\n",
    "            # return the matches along with the homograpy matrix\n",
    "            # and status of each matched point\n",
    "            return (matches, H, status)\n",
    "\n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high 4688\n",
      "High-Dirt-Full-Left-0.jpg High-Dirt-Full-Right-0.jpg\n",
      "High-Dirt-Full-Left-1.jpg High-Dirt-Full-Right-1.jpg\n",
      "High-Dirt-Full-Left-10.jpg High-Dirt-Full-Right-10.jpg\n",
      "High-Dirt-Full-Left-100.jpg High-Dirt-Full-Right-100.jpg\n",
      "High-Dirt-Full-Left-101.jpg High-Dirt-Full-Right-101.jpg\n",
      "High-Dirt-Full-Left-102.jpg High-Dirt-Full-Right-102.jpg\n",
      "High-Dirt-Full-Left-103.jpg High-Dirt-Full-Right-103.jpg\n",
      "High-Dirt-Full-Left-104.jpg High-Dirt-Full-Right-104.jpg\n",
      "High-Dirt-Full-Left-105.jpg High-Dirt-Full-Right-105.jpg\n",
      "High-Dirt-Full-Left-106.jpg High-Dirt-Full-Right-106.jpg\n",
      "High-Dirt-Full-Left-107.jpg High-Dirt-Full-Right-107.jpg\n",
      "High-Dirt-Full-Left-108.jpg High-Dirt-Full-Right-108.jpg\n",
      "High-Dirt-Full-Left-109.jpg High-Dirt-Full-Right-109.jpg\n",
      "High-Dirt-Full-Left-11.jpg High-Dirt-Full-Right-11.jpg\n",
      "High-Dirt-Full-Left-110.jpg High-Dirt-Full-Right-110.jpg\n",
      "High-Dirt-Full-Left-111.jpg High-Dirt-Full-Right-111.jpg\n",
      "High-Dirt-Full-Left-112.jpg High-Dirt-Full-Right-112.jpg\n",
      "High-Dirt-Full-Left-113.jpg High-Dirt-Full-Right-113.jpg\n",
      "High-Dirt-Full-Left-114.jpg High-Dirt-Full-Right-114.jpg\n",
      "High-Dirt-Full-Left-115.jpg High-Dirt-Full-Right-115.jpg\n",
      "High-Dirt-Full-Left-117.jpg High-Dirt-Full-Right-117.jpg\n",
      "High-Dirt-Full-Left-118.jpg High-Dirt-Full-Right-118.jpg\n",
      "High-Dirt-Full-Left-119.jpg High-Dirt-Full-Right-119.jpg\n",
      "High-Dirt-Full-Left-12.jpg High-Dirt-Full-Right-12.jpg\n",
      "High-Dirt-Full-Left-121.jpg High-Dirt-Full-Right-121.jpg\n",
      "High-Dirt-Full-Left-122.jpg High-Dirt-Full-Right-122.jpg\n",
      "High-Dirt-Full-Left-123.jpg High-Dirt-Full-Right-123.jpg\n",
      "High-Dirt-Full-Left-124.jpg High-Dirt-Full-Right-124.jpg\n",
      "High-Dirt-Full-Left-126.jpg High-Dirt-Full-Right-126.jpg\n",
      "High-Dirt-Full-Left-128.jpg High-Dirt-Full-Right-128.jpg\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) /tmp/build/80754af9/opencv-suite_1535558553474/work/modules/imgproc/src/imgwarp.cpp:2926: error: (-215:Assertion failed) (M0.type() == 5 || M0.type() == 6) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b2c987925518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mimg_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#cv2.imread(str(fname_right))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpan_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mout_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfname_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-324378ca4f27>\u001b[0m in \u001b[0;36mstitch\u001b[0;34m(self, images, ratio, reprojThresh)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# using the cached homography matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         result = cv2.warpPerspective(imageA, self.cachedH,\n\u001b[0;32m---> 42\u001b[0;31m             (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimageB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimageB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.2) /tmp/build/80754af9/opencv-suite_1535558553474/work/modules/imgproc/src/imgwarp.cpp:2926: error: (-215:Assertion failed) (M0.type() == 5 || M0.type() == 6) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n"
     ]
    }
   ],
   "source": [
    "img_dir = Path('/home/ajc/Documents/terrain-classification/image-data/01-single-rgb/')\n",
    "\n",
    "out_dir = Path('/home/ajc/Documents/terrain-classification/image-data/03-panoramas/')\n",
    "\n",
    "img_classes = ['high', 'medium', 'low']\n",
    "\n",
    "synth = ViewSynthisizer()\n",
    "\n",
    "num_dead = 0\n",
    "\n",
    "for clss in img_classes:\n",
    "    \n",
    "    img_fnames_left = sorted((img_dir / clss).glob('*eft-[0-9]*.jpg'))\n",
    "    img_fnames_right = sorted((img_dir / clss).glob('*ight-[0-9]*.jpg'))\n",
    "    \n",
    "    print(clss, len(img_fnames_right)*2)\n",
    "    \n",
    "    out_dir_class = out_dir / clss\n",
    "    out_dir_class.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for fname_left, fname_right in zip(img_fnames_left, img_fnames_right):\n",
    "\n",
    "        img_left = plt.imread(fname_left)# cv2.imread(str(fname_left))\n",
    "        img_right = plt.imread(fname_right)#cv2.imread(str(fname_right))\n",
    "\n",
    "        pan_frame = synth.stitch([img_left, img_right])\n",
    "        \n",
    "        out_fname = fname_left.name\n",
    "        out_fpath = out_dir_class / out_fname\n",
    "        \n",
    "        if pan_frame is None:\n",
    "            print(fname_left.name, fname_right.name)\n",
    "            num_dead += 1\n",
    "        \n",
    "        else:\n",
    "            plt.imsave(out_fpath, pan_frame)\n",
    "        \n",
    "#         break\n",
    "#     break\n",
    "\n",
    "print(num_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
